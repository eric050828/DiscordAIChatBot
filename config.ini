[default]
ollama_url = http://localhost:11434/api/generate
memory_path = memory
chat_history_length = 8

[model]
model_name = llama3
stream = False
mirostat = 0
mirostat_eta = 0.1
mirostat_tau = 5.0
num_ctx = 8192
repeat_last_n = 64
repeat_penalty = 1.1
temperature = 0.8
seed = 0
# stop = ["<|start_header_id|>", "<|end_header_id|>", "<|eot_id|>"]
tfs_z = 1
num_predict = -1
top_k = 40
top_p = 0.9

[logger]
folder = logs